{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Robustness of the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from google.colab import drive\n",
    "# import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Previous Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Architectures.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding small noise to the original data and Generating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adversarial Attack: testing the model's resilience to small but intentional perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_examples(X, model, epsilon=0.01):\n",
    "\n",
    "    # Generate predictions for the original data\n",
    "    original_predictions = model.predict(X)\n",
    "    original_classes = np.argmax(original_predictions, axis=1)  #the classes predicted for the original data\n",
    "\n",
    "    # Add small noise to the original data\n",
    "    noise = np.random.normal(0, epsilon, X.shape)\n",
    "    adversarial_X = X + noise   #the adversarially modified data\n",
    "\n",
    "    # Ensure the data still falls within valid MFCC range\n",
    "    adversarial_X = np.clip(adversarial_X, -1, 1)\n",
    "\n",
    "    # Generate predictions for the adversarial data\n",
    "    adversarial_predictions = model.predict(adversarial_X)\n",
    "    adversarial_classes = np.argmax(adversarial_predictions, axis=1)    #the classes predicted for the adversarial data\n",
    "\n",
    "\n",
    "    return adversarial_X, original_classes, adversarial_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for a single iteration of the cross validation\n",
    "##### fold1 - test\n",
    "##### fold2 and fold3 - validation\n",
    "##### fold3 - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_test(nn_type, mfcc, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units):\n",
    "    i = 1 #fold1 used for test\n",
    "\n",
    "    # Define test set paths\n",
    "    X_test_path = 'UrbanSound8K/audio/fold'+str(i)+'_4sec_mfccs_13/3D_array.npy'\n",
    "    Y_test_path = 'UrbanSound8K/audio/fold'+str(i)+'_label/3D_array.npy'\n",
    "\n",
    "    X_val_path1 = 'UrbanSound8K/audio/fold'+str((i+1-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "    Y_val_path1 = 'UrbanSound8K/audio/fold'+str((i+1-1) % 10 + 1)+'_label/3D_array.npy'\n",
    "    X_val_path2 = 'UrbanSound8K/audio/fold'+str((i+2-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "    Y_val_path2 = 'UrbanSound8K/audio/fold'+str((i+2-1) % 10 + 1)+'_label/3D_array.npy'\n",
    "\n",
    "    # Combine the validation paths\n",
    "    X_val_paths = [X_val_path1, X_val_path2]\n",
    "    Y_val_paths = [Y_val_path1, Y_val_path2]\n",
    "\n",
    "    # # Define training set paths (all remaining folds)\n",
    "    X_train_paths = ['UrbanSound8K/audio/fold'+str((j-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy' for j in range(i+3, i+10)]\n",
    "    Y_train_paths = ['UrbanSound8K/audio/fold'+str((j-1) % 10 + 1)+'_label/3D_array.npy' for j in range(i+3, i+10)]\n",
    "\n",
    "    # Define test set paths\n",
    "    X_test_path = f'UrbanSound8K/audio/fold{i}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "    Y_test_path = f'UrbanSound8K/audio/fold{i}_label/3D_array.npy'\n",
    "\n",
    "    # Define validation set paths (wrapping around if i+2 > 10)\n",
    "    X_val_path1 = f'UrbanSound8K/audio/fold{((i+1-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "    Y_val_path1 = f'UrbanSound8K/audio/fold{((i+1-1) % 10 + 1)}_label/3D_array.npy'\n",
    "    X_val_path2 = f'UrbanSound8K/audio/fold{((i+2-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "    Y_val_path2 = f'UrbanSound8K/audio/fold{((i+2-1) % 10 + 1)}_label/3D_array.npy'\n",
    "\n",
    "    # Combine the validation paths\n",
    "    X_val_paths = [X_val_path1, X_val_path2]\n",
    "    Y_val_paths = [Y_val_path1, Y_val_path2]\n",
    "\n",
    "    # Define training set paths (all remaining folds)\n",
    "    X_train_paths = [f'UrbanSound8K/audio/fold{((j-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy' for j in range(i+3, i+11)]\n",
    "    Y_train_paths = [f'UrbanSound8K/audio/fold{((j-1) % 10 + 1)}_label/3D_array.npy' for j in range(i+3, i+11)]\n",
    "\n",
    "    # Load the datasets from the paths\n",
    "    X_test = np.load(X_test_path)\n",
    "    Y_test = np.load(Y_test_path)\n",
    "\n",
    "\n",
    "    X_val1 = np.load(X_val_path1)\n",
    "    X_val2 = np.load(X_val_path2)\n",
    "    Y_val1 = np.load(Y_val_path1)\n",
    "    Y_val2 = np.load(Y_val_path2)\n",
    "\n",
    "    X_train = [np.load(path) for path in X_train_paths]\n",
    "    Y_train = [np.load(path) for path in Y_train_paths]\n",
    "\n",
    "\n",
    "    # Find the minimum size among all folds\n",
    "    min_size = min([X_test.shape[0], X_val1.shape[0], X_val2.shape[0]] + [x.shape[0] for x in X_train])\n",
    "\n",
    "    # Resize the data of each fold to the minimum size\n",
    "    X_test_resized = X_test[:min_size]\n",
    "    Y_test_resized = Y_test[:min_size]\n",
    "\n",
    "    X_val1_resized = X_val1[:min_size]\n",
    "    Y_val1_resized = Y_val1[:min_size]\n",
    "\n",
    "    X_val2_resized = X_val2[:min_size]\n",
    "    Y_val2_resized = Y_val2[:min_size]\n",
    "\n",
    "    X_train_resized = [x[:min_size] for x in X_train]\n",
    "    Y_train_resized = [y[:min_size] for y in Y_train]\n",
    "\n",
    "\n",
    "    # Combine the validation sets\n",
    "    X_test = X_test_resized\n",
    "    Y_test = Y_test_resized\n",
    "    X_val = np.concatenate((X_val1_resized, X_val2_resized), axis=2)\n",
    "    Y_val = np.concatenate((Y_val1_resized, Y_val2_resized), axis=2)\n",
    "    X_train = np.concatenate((X_train_resized[0], X_train_resized[1], X_train_resized[2], X_train_resized[3], X_train_resized[4], X_train_resized[5], X_train_resized[6]), axis=2)\n",
    "    Y_train = np.concatenate((Y_train_resized[0], Y_train_resized[1], Y_train_resized[2], Y_train_resized[3], Y_train_resized[4], Y_train_resized[5], Y_train_resized[6]), axis=2)\n",
    "\n",
    "    X_test = X_test.transpose(2,1,0)\n",
    "    Y_test = Y_test.transpose(2,1,0)\n",
    "    X_val = X_val.transpose(2,1,0)\n",
    "    Y_val = Y_val.transpose(2,1,0)\n",
    "    X_train = X_train.transpose(2,1,0)\n",
    "    Y_train = Y_train.transpose(2,1,0)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #Clear previous models\n",
    "    clear_session()  \n",
    "\n",
    "    # Call de neural network\n",
    "    input_shape = (751, mfcc)\n",
    "    num_classes = 10\n",
    "\n",
    "    #Neural Network Initialization\n",
    "    if nn_type == \"mlp\":\n",
    "        model = build_mlp_model(input_shape, num_classes, l2_lambda, n_hidden_units)\n",
    "\n",
    "    elif nn_type == \"bd_rnn\":\n",
    "        model = build_bd_rnn_model(input_shape, num_classes, l2_lambda, n_hidden_units)\n",
    "\n",
    "    else:\n",
    "        print(\"Introduce a valid neural network\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.summary() # print The neural network's architecture\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = legacy.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    #output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    \n",
    "    Y_train = np.squeeze(Y_train, axis=1)\n",
    "    Y_val = np.squeeze(Y_val, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "\n",
    "        #training dataset\n",
    "        X_train,\n",
    "        Y_train,\n",
    "\n",
    "        epochs=n_epochs,  # Number of epochs\n",
    "        batch_size=batch_size, # Number of samples per batch\n",
    "\n",
    "        #validation dataset\n",
    "        validation_data=(X_val, Y_val)\n",
    "    )\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------- Adding Robustness --------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Select a batch of test data for generating adversarial examples\n",
    "    batch_X_test = X_test[:batch_size]\n",
    "    batch_Y_test = Y_test[:batch_size]\n",
    "\n",
    "    print(batch_X_test.shape)\n",
    "    print(batch_Y_test.shape)\n",
    "\n",
    "    batch_Y_test = np.squeeze(batch_Y_test, axis=1)\n",
    "\n",
    "    if batch_Y_test.ndim == 3:\n",
    "        batch_Y_test = batch_Y_test[:, 0, :]\n",
    "\n",
    "    print(batch_Y_test.shape)  \n",
    "\n",
    "    # Generate adversarial examples\n",
    "    adv_X_test, original_classes, adv_classes = generate_adversarial_examples(batch_X_test, model)\n",
    "\n",
    "    # Evaluate the model on adversarial examples\n",
    "    adv_scores = model.evaluate(adv_X_test, batch_Y_test, verbose=0)\n",
    "    print(f\"Adversarial Test Accuracy: {adv_scores[1]}\")\n",
    "\n",
    "    # Analyze how many examples were successfully perturbed\n",
    "    successful_perturbations = np.sum(original_classes != adv_classes)\n",
    "    print(f\"Successfully perturbed examples: {successful_perturbations} out of {batch_size}\")\n",
    "\n",
    "    # Calculate the robustness as the percentage of successful perturbations\n",
    "    robustness = successful_perturbations / batch_size\n",
    "    print(f\"Robustness: {robustness}\")\n",
    "    percentage_successful_perturbations = successful_perturbations / batch_size * 100\n",
    "    print(f\"Percentage of successful perturbations: {percentage_successful_perturbations}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def robustness_main(nn_type, mfcc, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units):\n",
    "    # General Confusion Matrix\n",
    "    confusion_matrix_accumulated = np.zeros((10, 10))\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(1, 11):\n",
    "\n",
    "        # Define test set paths\n",
    "        X_test_path = 'UrbanSound8K/audio/fold'+str(i)+'_4sec_mfccs_13/3D_array.npy'\n",
    "        Y_test_path = 'UrbanSound8K/audio/fold'+str(i)+'_label/3D_array.npy'\n",
    "\n",
    "        X_val_path1 = 'UrbanSound8K/audio/fold'+str((i+1-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "        Y_val_path1 = 'UrbanSound8K/audio/fold'+str((i+1-1) % 10 + 1)+'_label/3D_array.npy'\n",
    "        X_val_path2 = 'UrbanSound8K/audio/fold'+str((i+2-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "        Y_val_path2 = 'UrbanSound8K/audio/fold'+str((i+2-1) % 10 + 1)+'_label/3D_array.npy'\n",
    "\n",
    "        # Combine the validation paths\n",
    "        X_val_paths = [X_val_path1, X_val_path2]\n",
    "        Y_val_paths = [Y_val_path1, Y_val_path2]\n",
    "\n",
    "        # # Define training set paths (all remaining folds)\n",
    "        X_train_paths = ['UrbanSound8K/audio/fold'+str((j-1) % 10 + 1)+f'_4sec_mfccs_{mfcc}/3D_array.npy' for j in range(i+3, i+10)]\n",
    "        Y_train_paths = ['UrbanSound8K/audio/fold'+str((j-1) % 10 + 1)+'_label/3D_array.npy' for j in range(i+3, i+10)]\n",
    "\n",
    "        # Define test set paths\n",
    "        X_test_path = f'UrbanSound8K/audio/fold{i}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "        Y_test_path = f'UrbanSound8K/audio/fold{i}_label/3D_array.npy'\n",
    "\n",
    "        # Define validation set paths (wrapping around if i+2 > 10)\n",
    "        X_val_path1 = f'UrbanSound8K/audio/fold{((i+1-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "        Y_val_path1 = f'UrbanSound8K/audio/fold{((i+1-1) % 10 + 1)}_label/3D_array.npy'\n",
    "        X_val_path2 = f'UrbanSound8K/audio/fold{((i+2-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy'\n",
    "        Y_val_path2 = f'UrbanSound8K/audio/fold{((i+2-1) % 10 + 1)}_label/3D_array.npy'\n",
    "\n",
    "        # Combine the validation paths\n",
    "        X_val_paths = [X_val_path1, X_val_path2]\n",
    "        Y_val_paths = [Y_val_path1, Y_val_path2]\n",
    "\n",
    "        # Define training set paths (all remaining folds)\n",
    "        X_train_paths = [f'UrbanSound8K/audio/fold{((j-1) % 10 + 1)}_4sec_mfccs_{mfcc}/3D_array.npy' for j in range(i+3, i+11)]\n",
    "        Y_train_paths = [f'UrbanSound8K/audio/fold{((j-1) % 10 + 1)}_label/3D_array.npy' for j in range(i+3, i+11)]\n",
    "\n",
    "        # Load the datasets from the paths\n",
    "        X_test = np.load(X_test_path)\n",
    "        Y_test = np.load(Y_test_path)\n",
    "\n",
    "\n",
    "        X_val1 = np.load(X_val_path1)\n",
    "        X_val2 = np.load(X_val_path2)\n",
    "        Y_val1 = np.load(Y_val_path1)\n",
    "        Y_val2 = np.load(Y_val_path2)\n",
    "\n",
    "        X_train = [np.load(path) for path in X_train_paths]\n",
    "        Y_train = [np.load(path) for path in Y_train_paths]\n",
    "\n",
    "\n",
    "        # Encontrar o tamanho mínimo entre todos os folds\n",
    "        min_size = min([X_test.shape[0], X_val1.shape[0], X_val2.shape[0]] + [x.shape[0] for x in X_train])\n",
    "\n",
    "        # Redimensionar os dados de cada fold para o tamanho mínimo\n",
    "        X_test_resized = X_test[:min_size]\n",
    "        Y_test_resized = Y_test[:min_size]\n",
    "\n",
    "        X_val1_resized = X_val1[:min_size]\n",
    "        Y_val1_resized = Y_val1[:min_size]\n",
    "\n",
    "        X_val2_resized = X_val2[:min_size]\n",
    "        Y_val2_resized = Y_val2[:min_size]\n",
    "\n",
    "        X_train_resized = [x[:min_size] for x in X_train]\n",
    "        Y_train_resized = [y[:min_size] for y in Y_train]\n",
    "\n",
    "\n",
    "        # Combine the validation sets\n",
    "        X_test = X_test_resized\n",
    "        Y_test = Y_test_resized\n",
    "        X_val = np.concatenate((X_val1_resized, X_val2_resized), axis=2)\n",
    "        Y_val = np.concatenate((Y_val1_resized, Y_val2_resized), axis=2)\n",
    "        X_train = np.concatenate((X_train_resized[0], X_train_resized[1], X_train_resized[2], X_train_resized[3], X_train_resized[4], X_train_resized[5], X_train_resized[6]), axis=2)\n",
    "        Y_train = np.concatenate((Y_train_resized[0], Y_train_resized[1], Y_train_resized[2], Y_train_resized[3], Y_train_resized[4], Y_train_resized[5], Y_train_resized[6]), axis=2)\n",
    "\n",
    "        X_test = X_test.transpose(2,1,0)\n",
    "        Y_test = Y_test.transpose(2,1,0)\n",
    "        X_val = X_val.transpose(2,1,0)\n",
    "        Y_val = Y_val.transpose(2,1,0)\n",
    "        X_train = X_train.transpose(2,1,0)\n",
    "        Y_train = Y_train.transpose(2,1,0)\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        clear_session()  # Clear the previous model\n",
    "        \n",
    "        # Call de neural network\n",
    "        input_shape = (751, mfcc)\n",
    "        num_classes = 10\n",
    "\n",
    "        #Neural Network Initialization\n",
    "        if nn_type == \"mlp\":\n",
    "            model = build_mlp_model(input_shape, num_classes, l2_lambda, n_hidden_units)\n",
    "\n",
    "        elif nn_type == \"rnn\":\n",
    "            model = build_rnn_model(input_shape, num_classes, l2_lambda, n_hidden_units)\n",
    "\n",
    "        else:\n",
    "            print(\"Introduce a valid neural network\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.summary() # print The neural network's architecture\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = legacy.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "        #output layer\n",
    "        model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        Y_train = np.squeeze(Y_train, axis=1)\n",
    "        Y_val = np.squeeze(Y_val, axis=1)\n",
    "\n",
    "\n",
    "        # Compile the model.\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "\n",
    "        history = model.fit(\n",
    "\n",
    "            #training dataset\n",
    "            X_train,\n",
    "            Y_train,\n",
    "\n",
    "            epochs=n_epochs,  # Number of epochs\n",
    "            batch_size=batch_size, # Number of samples per batch\n",
    "\n",
    "            #validation dataset\n",
    "            validation_data=(X_val, Y_val)\n",
    "        )\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------- Adding Robustness --------------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "        # Select a batch of test data for generating adversarial examples\n",
    "        batch_X_test = X_test[:batch_size]\n",
    "        batch_Y_test = Y_test[:batch_size]\n",
    "\n",
    "        print(batch_X_test.shape)\n",
    "        print(batch_Y_test.shape)\n",
    "\n",
    "        batch_Y_test = np.squeeze(batch_Y_test, axis=1)\n",
    "\n",
    "        if batch_Y_test.ndim == 3:\n",
    "            batch_Y_test = batch_Y_test[:, 0, :]\n",
    "\n",
    "        print(batch_Y_test.shape)  \n",
    "\n",
    "        # Generate adversarial examples\n",
    "        adv_X_test, original_classes, adv_classes = generate_adversarial_examples(batch_X_test, model)\n",
    "\n",
    "        # Evaluate the model on adversarial examples\n",
    "        adv_scores = model.evaluate(adv_X_test, batch_Y_test, verbose=0)\n",
    "        print(f\"Adversarial Test Accuracy: {adv_scores[1]}\")\n",
    "\n",
    "        # Analyze how many examples were successfully perturbed\n",
    "        successful_perturbations = np.sum(original_classes != adv_classes)\n",
    "        print(f\"Successfully perturbed examples: {successful_perturbations} out of {batch_size}\")\n",
    "\n",
    "\n",
    "        # Calculate the robustness as the percentage of successful perturbations\n",
    "        robustness = successful_perturbations / batch_size\n",
    "        print(f\"Robustness: {robustness}\")\n",
    "        percentage_successful_perturbations = successful_perturbations / batch_size * 100\n",
    "        print(f\"Percentage of successful perturbations: {percentage_successful_perturbations}%\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        Y_train_classes = np.squeeze(Y_train).argmax(axis=-1) if Y_train.ndim == 3 else Y_train\n",
    "        Y_val_classes = np.squeeze(Y_val).argmax(axis=-1) if Y_val.ndim == 3 else Y_val\n",
    "\n",
    "        # Generate predictions\n",
    "        Y_pred = model.predict(X_test)  # Predicted probabilities\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        Y_pred_classes = np.argmax(Y_pred, axis=-1)  # Convert probabilities to class labels\n",
    "\n",
    "        # Flatten the one-hot encoded labels to 1D if they are 3D for Y_test\n",
    "        Y_test_classes = np.squeeze(Y_test).argmax(axis=-1) if Y_test.ndim == 3 else Y_test\n",
    "\n",
    "        # Make sure the confusion matrix function is correctly imported\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        conf_matrix = confusion_matrix(Y_test_classes, Y_pred_classes)\n",
    "\n",
    "        # Add the current confusion matrix to the accumulated confusion matrix\n",
    "        confusion_matrix_accumulated += conf_matrix\n",
    "\n",
    "        # Evaluate the model, ensuring the Y_test used here matches the format expected by the model\n",
    "        scores = model.evaluate(X_test, np.squeeze(Y_test), verbose=0)\n",
    "        print(f\"Test accuracy for fold {i}:\", scores[1])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = precision_score(Y_test_classes, Y_pred_classes, average='macro')\n",
    "        recall = recall_score(Y_test_classes, Y_pred_classes, average='macro')\n",
    "\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "\n",
    "        accuracies.append(scores[1])\n",
    "        print(f\"Test accuracy for fold {i}:\", scores[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for a single iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 13 coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 13 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best Hyperparameters given by tensorboard - The best score: 0.26345932483673096\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.8\n",
    "l2_lambda = 0.0001\n",
    "n_hidden_units = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 9763)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                624896    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 633866 (2.42 MB)\n",
      "Trainable params: 633866 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 2.3572 - accuracy: 0.1123 - val_loss: 2.3596 - val_accuracy: 0.1103\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.3397 - accuracy: 0.1183 - val_loss: 2.3577 - val_accuracy: 0.1103\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.3373 - accuracy: 0.1193 - val_loss: 2.3542 - val_accuracy: 0.1103\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 0s 3ms/step - loss: 2.3321 - accuracy: 0.1189 - val_loss: 2.3473 - val_accuracy: 0.1103\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.3127 - accuracy: 0.1332 - val_loss: 2.2831 - val_accuracy: 0.2035\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.2890 - accuracy: 0.1578 - val_loss: 2.2772 - val_accuracy: 0.2107\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.2733 - accuracy: 0.1667 - val_loss: 2.2810 - val_accuracy: 0.1798\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.2706 - accuracy: 0.1758 - val_loss: 2.2550 - val_accuracy: 0.2074\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.2637 - accuracy: 0.1835 - val_loss: 2.3020 - val_accuracy: 0.1423\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 0s 2ms/step - loss: 2.2557 - accuracy: 0.1866 - val_loss: 2.2593 - val_accuracy: 0.2212\n",
      "(32, 751, 13)\n",
      "(32, 1, 10)\n",
      "(32, 10)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Adversarial Test Accuracy: 0.71875\n",
      "Successfully perturbed examples: 8 out of 32\n",
      "Robustness: 0.25\n",
      "Percentage of successful perturbations: 25.0%\n"
     ]
    }
   ],
   "source": [
    "robustness_test(\"mlp\", 13, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 751, 128)          39936     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140042 (547.04 KB)\n",
      "Trainable params: 140042 (547.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 213s 1s/step - loss: 2.3341 - accuracy: 0.1193 - val_loss: 2.3283 - val_accuracy: 0.1131\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 218s 1s/step - loss: 2.3329 - accuracy: 0.1165 - val_loss: 2.3270 - val_accuracy: 0.1153\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 240s 1s/step - loss: 2.3323 - accuracy: 0.1143 - val_loss: 2.3259 - val_accuracy: 0.1180\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 258s 1s/step - loss: 2.3311 - accuracy: 0.1161 - val_loss: 2.3249 - val_accuracy: 0.1175\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 259s 1s/step - loss: 2.3303 - accuracy: 0.1184 - val_loss: 2.3240 - val_accuracy: 0.1197\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 261s 1s/step - loss: 2.3300 - accuracy: 0.1175 - val_loss: 2.3231 - val_accuracy: 0.1197\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 263s 1s/step - loss: 2.3286 - accuracy: 0.1168 - val_loss: 2.3224 - val_accuracy: 0.1213\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 229s 1s/step - loss: 2.3283 - accuracy: 0.1130 - val_loss: 2.3215 - val_accuracy: 0.1307\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 224s 1s/step - loss: 2.3273 - accuracy: 0.1209 - val_loss: 2.3207 - val_accuracy: 0.1324\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 225s 1s/step - loss: 2.3267 - accuracy: 0.1170 - val_loss: 2.3201 - val_accuracy: 0.1291\n",
      "(32, 751, 13)\n",
      "(32, 1, 10)\n",
      "(32, 10)\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "Adversarial Test Accuracy: 0.0\n",
      "Successfully perturbed examples: 4 out of 32\n",
      "Robustness: 0.125\n",
      "Percentage of successful perturbations: 12.5%\n"
     ]
    }
   ],
   "source": [
    "robustness_test(\"bd_rnn\", 13, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 25 coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best Hyperparameters given by tensorboard - The best score: 0.26345932483673096\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.9\n",
    "l2_lambda = 0.0001\n",
    "n_hidden_units = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 18775)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1201664   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1210634 (4.62 MB)\n",
      "Trainable params: 1210634 (4.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "189/189 [==============================] - 1s 4ms/step - loss: 2.3618 - accuracy: 0.1039 - val_loss: 2.3963 - val_accuracy: 0.1076\n",
      "Epoch 2/5\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 2.3705 - accuracy: 0.1189 - val_loss: 2.3660 - val_accuracy: 0.1076\n",
      "Epoch 3/5\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 2.3460 - accuracy: 0.1199 - val_loss: 2.3215 - val_accuracy: 0.1103\n",
      "Epoch 4/5\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 2.3241 - accuracy: 0.1161 - val_loss: 2.3092 - val_accuracy: 0.1103\n",
      "Epoch 5/5\n",
      "189/189 [==============================] - 1s 3ms/step - loss: 2.3127 - accuracy: 0.1165 - val_loss: 2.3007 - val_accuracy: 0.1103\n",
      "(32, 751, 25)\n",
      "(32, 1, 10)\n",
      "(32, 10)\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Adversarial Test Accuracy: 0.0\n",
      "Successfully perturbed examples: 2 out of 32\n",
      "Robustness: 0.0625\n",
      "Percentage of successful perturbations: 6.25%\n"
     ]
    }
   ],
   "source": [
    "robustness_test(\"mlp\", 25, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 751, 128)          46080     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146186 (571.04 KB)\n",
      "Trainable params: 146186 (571.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "189/189 [==============================] - 208s 1s/step - loss: 2.3385 - accuracy: 0.1132 - val_loss: 2.3256 - val_accuracy: 0.0965\n",
      "Epoch 2/5\n",
      "189/189 [==============================] - 224s 1s/step - loss: 2.3279 - accuracy: 0.1166 - val_loss: 2.3179 - val_accuracy: 0.1175\n",
      "Epoch 3/5\n",
      "189/189 [==============================] - 225s 1s/step - loss: 2.3200 - accuracy: 0.1266 - val_loss: 2.3116 - val_accuracy: 0.1103\n",
      "Epoch 4/5\n",
      "189/189 [==============================] - 225s 1s/step - loss: 2.3136 - accuracy: 0.1266 - val_loss: 2.3061 - val_accuracy: 0.1153\n",
      "Epoch 5/5\n",
      "189/189 [==============================] - 225s 1s/step - loss: 2.3081 - accuracy: 0.1390 - val_loss: 2.3010 - val_accuracy: 0.1120\n",
      "(32, 751, 25)\n",
      "(32, 1, 10)\n",
      "(32, 10)\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x109b6a280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 513ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "Adversarial Test Accuracy: 0.25\n",
      "Successfully perturbed examples: 29 out of 32\n",
      "Robustness: 0.90625\n",
      "Percentage of successful perturbations: 90.625%\n"
     ]
    }
   ],
   "source": [
    "robustness_test(\"bd_rnn\", 25, n_epochs, batch_size, learning_rate, momentum, l2_lambda, n_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
